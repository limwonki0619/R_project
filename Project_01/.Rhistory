Encoding(rownames(tdm.matrix), "UTF-8")
Encoding(rownames(tdm.matrix)) <- "UTF-8"
rownames(tdm.matrix)[1:100]
cps <- Corpus(VectorSource(C_group$리뷰))
cps
cps
words <- function(word){
text <- as.character(word)
text2 <- sapply(text, extractNoun, USE.NAMES = F)
text3 <- unlist(text2) %>% str_match("([가-힣]+)")
text4 <- Filter(function(x){nchar(x) >= 2 & nchar(x) <= 7},text4)
}
tdm <- TermDocumentMatrix(cps, control=list(tokenize=words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
wordLengths=c(4, 10),
weighting=weightBin))
dim(tdm)
tdm.matrix <- as.matrix(tdm)
Encoding(rownames(tdm.matrix)) <- "UTF-8"
rownames(tdm.matrix)[1:100]
C_group$리뷰
words(C_group$리뷰)
t<-words(C_group$리뷰)
t
tdm <- TermDocumentMatrix(cps, control=list(tokenize=words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
weighting=weightBin))
dim(tdm)
tdm.matrix <- as.matrix(tdm)
Encoding(rownames(tdm.matrix)) <- "UTF-8"
rownames(tdm.matrix)[1:100]
tdm <- TermDocumentMatrix(cps, control=list(tokenize=words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
weighting=weightBin))
dim(tdm)
tdm.matrix <- as.matrix(tdm)
rownames(tdm.matrix)[1:100]
tdm
tdm <- TermDocumentMatrix(cps, control=list(tokenize=words,   ## token 분류시 활용할 함수명 지정
weighting=weightBin))
dim(tdm)
tdm.matrix <- as.matrix(tdm)
rownames(tdm.matrix)[1:100]
C_review
tdm.matrix
Encoding(tdm$dimnames$Terms) = 'UTF-8'
dim(tdm)
rownames(tdm.matrix)[1:100]
library(KoNLP)
library(KoNLP)
cps <- Corpus(VectorSource(C_group$리뷰))
Encoding(tdm$dimnames$Terms) = 'UTF-8'
dim(tdm)
rownames(tdm.matrix)[1:100]
Encoding(rownames(tdm.matrix)) <- "UTF-8"
rownames(tdm.matrix)[1:100]
Encoding(rownames(tdm.matrix)) <- "UTF-8"
rownames(tdm.matrix)[1:100]
tdm$dimnames$Docs
tdm$dimnames$Terms
Encoding(tdm$dimnames$Terms) <- 'UTF-8'
dim(tdm)
rownames(tdm.matrix)[1:100]
# 수집된 데이터 불러오기 --------------------------------
OG_data <- read.xlsx2("네이버_영화_리뷰_알라딘.xlsx", sheetIndex=1, header=T, stringsAsFactors=F)
# 필요패키지 설치 --------------------------------------
library(xlsx)
# 수집된 데이터 불러오기 --------------------------------
OG_data <- read.xlsx2("네이버_영화_리뷰_알라딘.xlsx", sheetIndex=1, header=T, stringsAsFactors=F)
dataset <- OG_data # 원본데이터 복사 (엑셀파일 불러올 때 메모리오류로 인해 에러가 자주남) # options(java.parameters = "-Xmx7168m")
# 데이터 형태 맞춰주기 ---------------------------------
dataset$점수 <- as.numeric(dataset$점수)
dataset$공감 <- as.numeric(dataset$공감)
dataset$비공감 <- as.numeric(dataset$비공감)
dataset$날짜 <- ymd(dataset$날짜)
dataset$시간 <- factor(dataset$시간, levels=c(0:23), order=T)
dataset$요일 <- factor(dataset$요일, levels=c("월","화","수","목","금","토","일"), order=T)
dataset$평점그룹 <- ifelse(dataset$점수 >= 9, "A", ifelse(dataset$점수 <= 5, "C", "B"))
dataset$주 <- ifelse(dataset$요일 %in% c("토","일"), "주말", "주중")
C_group <- filter(dataset, 평점그룹 == "C")
C_group <- filter(dataset, 평점그룹 == "C")
C_group <- filter(dataset, 평점그룹 == "C")
C_group <- filter(dataset, 평점그룹 == "C")
B_group <- filter(dataset, 평점그룹 == "B")
# 평점이 낮은 그룹 시간대별 평점평균 및 버즈량 -------------------------------------------
A_group <- filter(dataset, 평점그룹 == "A")
# 수집된 데이터 불러오기 --------------------------------
OG_data <- read.xlsx2("네이버_영화_리뷰_알라딘.xlsx", sheetIndex=1, header=T, stringsAsFactors=F)
# 필요패키지 설치 --------------------------------------
library(xlsx)
# 수집된 데이터 불러오기 --------------------------------
OG_data <- read.xlsx2("네이버_영화_리뷰_알라딘.xlsx", sheetIndex=1, header=T, stringsAsFactors=F)
dataset <- OG_data # 원본데이터 복사 (엑셀파일 불러올 때 메모리오류로 인해 에러가 자주남) # options(java.parameters = "-Xmx7168m")
# 워드클라우드 --------------------------------------------
library(rJava)
library(KoNLP)
# 워드클라우드 --------------------------------------------
library(rJava)
library(KoNLP)
# 워드클라우드 --------------------------------------------
install.packages("rJava")
install.packages("rJava")
library(rJava)
library(KoNLP)
library(dplyr)
library(stringr)
library(wordcloud2)
library(reshape2)
useSejongDic() # 세종사전 호출
useSejongDic() # 세종사전 호출
C_group <- filter(dataset, 평점그룹 == "C")
dataset$점수 <- as.numeric(dataset$점수)
dataset$공감 <- as.numeric(dataset$공감)
dataset$비공감 <- as.numeric(dataset$비공감)
dataset$날짜 <- ymd(dataset$날짜)
dataset$시간 <- factor(dataset$시간, levels=c(0:23), order=T)
dataset$요일 <- factor(dataset$요일, levels=c("월","화","수","목","금","토","일"), order=T)
dataset$평점그룹 <- ifelse(dataset$점수 >= 9, "A", ifelse(dataset$점수 <= 5, "C", "B"))
dataset$주 <- ifelse(dataset$요일 %in% c("토","일"), "주말", "주중")
A_group <- filter(dataset, 평점그룹 == "A")
B_group <- filter(dataset, 평점그룹 == "B")
C_group <- filter(dataset, 평점그룹 == "C")
cps <- Corpus(VectorSource(C_group$리뷰))
library(tm)
cps <- Corpus(VectorSource(C_group$리뷰))
C_review
words <- function(word){
text <- as.character(word)
text2 <- sapply(text, extractNoun, USE.NAMES = F)
text3 <- unlist(text2) %>% str_match("([가-힣]+)")
text4 <- Filter(function(x){nchar(x) >= 2 & nchar(x) <= 7},text4)
}
t<-words(C_group$리뷰)
words <- function(word){
text <- as.character(word)
text2 <- sapply(text, extractNoun, USE.NAMES = F)
text3 <- unlist(text2) %>% str_match("([가-힣]+)")
text4 <- Filter(function(x){nchar(x) >= 2 & nchar(x) <= 7},text3)
}
t<-words(C_group$리뷰)
tdm <- TermDocumentMatrix(cps, control=list(tokenize=words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
weighting=weightBin))
dim(tdm)
tdm.matrix <- as.matrix(tdm)
rownames(tdm.matrix)[1:100]
Encoding(tdm$dimnames$Terms) <- 'UTF-8'
tdm$dimnames$Terms
dim(tdm)
rownames(tdm.matrix)[1:100]
Encoding(rownames(tdm.matrix)) <- "UTF-8"
rownames(tdm.matrix)[1:100]
tdm <- TermDocumentMatrix(cps, control=list(tokenize=words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
weighting=weightBin))
tdm
ko.words <- function(doc){
d <- as.character(doc)
pos <- paste(SimplePos09(d))
extracted <- str_match(pos, '([가-힣]+)/[NP]')
keyword <- extracted[,2]
keyword[!is.na(keyword)]
}
tdm <- TermDocumentMatrix(cps, control=list(tokenize=ko.words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
weighting=weightBin))
Encoding(tdm$dimnames$Terms) <- 'UTF-8'
tdm$dimnames$Terms
Encoding(rownames(tdm.matrix)) <- "UTF-8"
rownames(tdm.matrix)[1:100]
ko.words <- function(doc){
d <- as.character(doc)
pos <- paste(SimplePos09(d))
extracted <- str_match(pos, '([가-힣]+)/[NP]')
keyword <- extracted[,2]
keyword[!is.na(keyword)]
}
tdm <- TermDocumentMatrix(cps, control=list(tokenize=ko.words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
weighting=weightBin))
tdm
Encoding(tdm$dimnames$Terms) <- 'UTF-8'
tdm$dimnames$Terms
## 점수대(그룹)별 워드클라우드 --------------------------------------
clean <- function(text) {
text <- as.character(text)
text2 <- sapply(text, extractNoun, USE.NAMES = F)
text3 <- unlist(text2) %>% str_match("([가-힣]+)")
text4 <- Filter(function(x){nchar(x) >= 2 & nchar(x) <= 7},text3)
}
C_review <- clean(C_group$리뷰) # 5점이하
C_review
install.packages("tm")
install.packages("tm")
library(tm)
cps <- Corpus(VectorSource(C_group$리뷰))
ko.words <- function(doc){
d <- as.character(doc)
pos <- paste(SimplePos09(d))
extracted <- str_match(pos, '([가-힣]+)/[NP]')
keyword <- extracted[,2]
keyword[!is.na(keyword)]
}
tdm <- TermDocumentMatrix(cps, control=list(tokenize=ko.words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
weighting=weightBin))
tdm$dimnames$Terms
library(stringr)
words <- function(word){
text <- as.character(word)
text2 <- sapply(text, extractNoun, USE.NAMES = F)
text3 <- str_match(text2, "([가-힣]+)")
text4 <- Filter(function(x){nchar(x) >= 2 & nchar(x) <= 7}, text3)
gsub_txt <- readLines("gsubfile.txt")
for (i in 1:length(gsub_txt)) {
text5  <- gsub((gsub_txt[i]),"",text4)
}
}
tdm <- TermDocumentMatrix(cps, control=list(tokenize=words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
wordLengths=c(4, 10),
weighting=weightBin))
tdm$dimnames$Terms
dim(tdm)
tdm.matrix <- as.matrix(tdm)
rownames(tdm.matrix)[1:100]
word.count <- rowSums(tdm.matrix)  ##각 단어별 합계를 구함
word.order <- order(word.count, decreasing=T)  #다음으로 단어들을 쓰인 횟수에 따라 내림차순으로 정렬
freq.words <- tdm.matrix[word.order[1:20], ] #Term Document Matrix에서 자주 쓰인 단어 상위 20개에 해당하는 것만 추출
co.matrix <- freq.words %*% t(freq.words)  #행렬의 곱셈을 이용해 Term Document Matrix를 Co-occurence Matrix로 변경
install.packages("qgraph")
library(qgraph)
qgraph(co.matrix,
labels=rownames(co.matrix),   ##label 추가
diag=F,                       ## 자신의 관계는 제거함
layout='spring',              ##노드들의 위치를 spring으로 연결된 것 처럼 관련이 강하면 같이 붙어 있고 없으면 멀리 떨어지도록 표시됨
vsize=log(diag(co.matrix))*2) ##diag는 matrix에서 대각선만 뽑는 것임. 즉 그 단어가 얼마나 나왔는지를 알 수 있음. vsize는 그 크기를 결정하는데 여기 인자값으로 단어가 나온 숫자를 넘겨주는 것임. log를 취한것은 단어 크기의 차이가 너무 커서 log를 통해서 그 차이를 좀 줄여준것임.
words <- function(word){
text <- as.character(word)
text2 <- sapply(text, extractNoun, USE.NAMES = F)
text3 <- str_match(text2, "([가-힣]+)")
text4 <- Filter(function(x){nchar(x) >= 2 & nchar(x) <= 7}, text3)
gsub_txt <- readLines("gsubfile_C.txt")
for (i in 1:length(gsub_txt)) {
text5  <- gsub((gsub_txt[i]),"",text4)
}
}
tdm <- TermDocumentMatrix(cps, control=list(tokenize=words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
wordLengths=c(4, 10),
weighting=weightBin))
word.count <- rowSums(tdm.matrix)  ##각 단어별 합계를 구함
word.order <- order(word.count, decreasing=T)  #다음으로 단어들을 쓰인 횟수에 따라 내림차순으로 정렬
freq.words <- tdm.matrix[word.order[1:20], ] #Term Document Matrix에서 자주 쓰인 단어 상위 20개에 해당하는 것만 추출
co.matrix <- freq.words %*% t(freq.words)  #행렬의 곱셈을 이용해 Term Document Matrix를 Co-occurence Matrix로 변경
word.order
freq.words
co.matrix
words <- function(word){
text <- as.character(word)
text2 <- sapply(text, extractNoun, USE.NAMES = F)
text3 <- str_match(text2, "([가-힣]+)")
text4 <- Filter(function(x){nchar(x) >= 2 & nchar(x) <= 7}, text3)
gsub_txt <- readLines("gsubfile_C.txt")
for (i in 1:length(gsub_txt)) {
text4  <- gsub((gsub_txt[i]),"",text4)
}
}
tdm <- TermDocumentMatrix(cps, control=list(tokenize=words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
wordLengths=c(4, 10),
weighting=weightBin))
tdm.matrix <- as.matrix(tdm)
rownames(tdm.matrix)[1:100]
word.count <- rowSums(tdm.matrix)  ##각 단어별 합계를 구함
word.order <- order(word.count, decreasing=T)  #다음으로 단어들을 쓰인 횟수에 따라 내림차순으로 정렬
freq.words <- tdm.matrix[word.order[1:20], ] #Term Document Matrix에서 자주 쓰인 단어 상위 20개에 해당하는 것만 추출
co.matrix <- freq.words %*% t(freq.words)  #행렬의 곱셈을 이용해 Term Document Matrix를 Co-occurence Matrix로 변경
qgraph(co.matrix,
labels=rownames(co.matrix),   ##label 추가
diag=F,                       ## 자신의 관계는 제거함
layout='spring',              ##노드들의 위치를 spring으로 연결된 것 처럼 관련이 강하면 같이 붙어 있고 없으면 멀리 떨어지도록 표시됨
vsize=log(diag(co.matrix))*2) ##diag는 matrix에서 대각선만 뽑는 것임. 즉 그 단어가 얼마나 나왔는지를 알 수 있음. vsize는 그 크기를 결정하는데 여기 인자값으로 단어가 나온 숫자를 넘겨주는 것임. log를 취한것은 단어 크기의 차이가 너무 커서 log를 통해서 그 차이를 좀 줄여준것임.
gsub_txt
text4
ko.words <- function(doc){
d <- as.character(doc)
pos <- paste(SimplePos09(d))
extracted <- str_match(pos, '([가-힣]+)/[NP]')
keyword <- extracted[,2]
keyword[!is.na(keyword)]
}
cps <- Corpus(VectorSource(C_group$리뷰))
tdm <- TermDocumentMatrix(cps,
control=list(tokenize=ko.words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
wordLengths=c(2, 6),
weighting=weightBin))
dim(tdm)
tdm.matrix <- as.matrix(tdm)
rownames(tdm.matrix)[1:100]
word.count <- rowSums(tdm.matrix)  ##각 단어별 합계를 구함
word.order <- order(word.count, decreasing=T)  #다음으로 단어들을 쓰인 횟수에 따라 내림차순으로 정렬
freq.words <- tdm.matrix[word.order[1:20], ] #Term Document Matrix에서 자주 쓰인 단어 상위 20개에 해당하는 것만 추출
co.matrix <- freq.words %*% t(freq.words)  #행렬의 곱셈을 이용해 Term Document Matrix를 Co-occurence Matrix로 변경
install.packages("qgraph")
library(qgraph)
qgraph(co.matrix,
labels=rownames(co.matrix),   ##label 추가
diag=F,                       ## 자신의 관계는 제거함
layout='spring',              ##노드들의 위치를 spring으로 연결된 것 처럼 관련이 강하면 같이 붙어 있고 없으면 멀리 떨어지도록 표시됨
vsize=log(diag(co.matrix))*2) ##diag는 matrix에서 대각선만 뽑는 것임. 즉 그 단어가 얼마나 나왔는지를 알 수 있음. vsize는 그 크기를 결정하는데 여기 인자값으로 단어가 나온 숫자를 넘겨주는 것임. log를 취한것은 단어 크기의 차이가 너무 커서 log를 통해서 그 차이를 좀 줄여준것임.
tdm <- TermDocumentMatrix(cps,
control=list(tokenize=ko.words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
wordLengths=c(2, 6),
gsub_txt <- readLines("gsubfile_C.txt"),
for (i in 1:length(gsub_txt)) {
A_text  <- gsub((gsub_txt[i]),"",A_text)
}
weighting=weightBin))
tdm <- TermDocumentMatrix(cps,
control=list(tokenize=ko.words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
wordLengths=c(2, 6),
gsub_txt <- readLines("gsubfile_C.txt"),
for (i in 1:length(gsub_txt)) {
A_text  <- gsub((gsub_txt[i]),"",A_text)
},
weighting=weightBin))
tdm <- TermDocumentMatrix(cps,
control=list(tokenize=ko.words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
wordLengths=c(2, 6),
=weightBin))
library(rJava)
library(KoNLP)
library(xlsx)
library(lubridate)
library(dplyr)
library(stringr)
library(wordcloud2)
library(reshape2)
library(extrafont)
windowsFonts(dohyeon=windowsFont("BM DoHyeon"))
windowsFonts(jalnan=windowsFont("Jalnan"))
library(ggplot2)
A_text <- readLines("알라딘_리뷰_전처리_후_A.txt")
head(sort(table(A_text), decreasing = T),50)
A_text <- gsub("재밌어요","재미", A_text)
A_text <- gsub("재밌고","재미", A_text)
A_text <- gsub("재밌게","재미", A_text)
A_text <- gsub("쟈스민","자스민", A_text)
A_text <- gsub("지니가","지니", A_text)
A_text <- gsub("윌스미스가","윌스미스", A_text)
A_text <- gsub("월스미스","윌스미스", A_text)
A_text <- gsub("관람객너무","관람객", A_text)
gsub_txt <- readLines("gsubfile_A.txt")
for (i in 1:length(gsub_txt)) {
A_text  <- gsub((gsub_txt[i]),"",A_text)
}
A_text <- A_text[A_text != ""]
A_wc <-head(sort(table(A_text), decreasing = T),50)
wordcloud2(A_wc, size=0.7,
col="random-light", backgroundColor="black",
fontFamily='나눔바른고딕')
wordcloud2(A_wc, size=0.7,
col="random-light", backgroundColor="black",
fontFamily='나눔바른고딕')
C_wc
# 워드클라우드 생성
wordcloud2(C_wc, size=0.6,
col="random-light", backgroundColor="black",
fontFamily='나눔바른고딕')
# C 그룹의 워드클라우드
C_text <- readLines("알라딘_리뷰_전처리_후_C.txt")
C_text <- gsub("애니메이션을","애니메이션",C_text)
C_text <- gsub("애니메이션이","애니메이션",C_text)
gsub_txt <- readLines("gsubfile_C.txt")
for (i in 1:length(gsub_txt)) {
C_text  <- gsub((gsub_txt[i]),"",C_text)
}
C_text <- C_text[C_text != ""]
C_wc <-head(sort(table(C_text), decreasing = T),50)
# 워드클라우드 생성
wordcloud2(C_wc, size=0.6,
col="random-light", backgroundColor="black",
fontFamily='나눔바른고딕')
# 워드클라우드 생성
wordcloud2(C_wc, size=0.6,
col="random-light", backgroundColor="black",
fontFamily='나눔바른고딕')
library(rJava)
library(KoNLP)
library(xlsx)
library(lubridate)
library(stringr)
library(wordcloud2)
library(reshape2)
library(extrafont)
windowsFonts(dohyeon=windowsFont("BM DoHyeon"))
windowsFonts(jalnan=windowsFont("Jalnan"))
library(ggplot2)
library(rvest)
library(dplyr)
trim <- function(text){gsub("\\s+","",text)} # str_trim(text, c("both"))와 같음
# 수집된 데이터 불러오기 --------------------------------
OG_data <- read.xlsx2("네이버_영화_리뷰_알라딘.xlsx", sheetIndex=1, header=T, stringsAsFactors=F)
dataset <- OG_data # 원본데이터 복사 (엑셀파일 불러올 때 메모리오류로 인해 에러가 자주남) # options(java.parameters = "-Xmx7168m")
dataset$점수 <- as.numeric(dataset$점수)
dataset$공감 <- as.numeric(dataset$공감)
dataset$비공감 <- as.numeric(dataset$비공감)
dataset$날짜 <- ymd(dataset$날짜)
dataset$시간 <- factor(dataset$시간, levels=c(0:23), order=T)
dataset$요일 <- factor(dataset$요일, levels=c("월","화","수","목","금","토","일"), order=T)
dataset$평점그룹 <- ifelse(dataset$점수 >= 9, "A", ifelse(dataset$점수 <= 5, "C", "B"))
dataset$주 <- ifelse(dataset$요일 %in% c("토","일"), "주말", "주중")
str(dataset)
dis <-dataset %>%
group_by(점수) %>%
summarise(리뷰수 = n()) %>% as.data.frame()
ggplot(dis, aes(x=점수, y=리뷰수)) +
geom_bar(stat="identity", fill=rainbow(10), position = "stack") +
geom_text(aes(y=리뷰수+500, label=리뷰수), family='dohyeon', size=3.5) +
scale_x_continuous(breaks = c(seq(0,10,1))) +
theme_bw(base_size = 12, base_family = "jalnan") +
labs(title = "점수별 리뷰 분포", y = "리뷰 수(단위:건)") +
theme(plot.title = element_text(hjust = 0.5))
# 일별 평점 데이터 생성
all_date_point <- dataset %>%
group_by(날짜, 요일) %>%
summarise(date_mean_point = mean(점수),
공감수 = sum(공감),
비공감수 = sum(비공감),
댓글수 = n())
# 일별 평점 평균 변화
ggplot(all_date_point,aes(x=날짜)) +
geom_line(aes(y=date_mean_point), color="grey20", linetype=2) +
geom_point(aes(y=date_mean_point), color=rainbow(43), size=3, alpha=.8) +
geom_text(aes(y=date_mean_point+0.04, label=round(date_mean_point,2)), family="dohyeon", size=3) +
coord_cartesian(ylim = c(9, 9.8)) +
theme_bw(base_family = "jalnan", base_size = 12) +
labs(y="평균평점", title="일별 평점평균 변화", subtitle="2019-05-23 ~ 2019-07-04") +
theme(legend.title = element_blank(),
plot.title = element_text(hjust=0.5, size=20, family = "jalnan"),
plot.subtitle = element_text(hjust=0.5, color="grey20"),
axis.text.x = element_text(angle = 90, vjust=0.5)) +
scale_x_date(breaks = all_date_point$날짜, labels=str_sub(all_date_point$날짜,7,10))
# 일별 댓글 수
ggplot(all_date_point, aes(x=날짜, y=댓글수)) +
geom_col(fill="antiquewhite", alpha=0.7, color="grey20") +
geom_text(aes(y=댓글수+30, label=댓글수), color='grey20', family="jalnan", size=3) +
geom_text(data = filter(all_date_point, 요일 %in% c("토", "일")), aes(y=100, label=요일), color="red", family="jalnan") +
theme_bw(base_family = "jalnan", base_size = 12) +
labs(title="일별 댓글 수",y="댓글 수(단위:건)", subtitle="2019-05-23 ~ 2019-07-04") +
theme(plot.title = element_text(hjust=0.5),
plot.subtitle = element_text(hjust=0.5, color="grey20"),
axis.text.x = element_text(angle = 90, vjust=0.5),
axis.text.y = element_blank()) +
scale_x_date(breaks = all_date_point$날짜, labels=str_sub(all_date_point$날짜,7,10))
# 요일별 평점 데이터 생성
wday_point <- dataset %>%
group_by(요일) %>%
summarise(date_mean_point = mean(점수),
공감수 = sum(공감),
비공감수 = sum(비공감),
댓글수 = n())
# 요일별 댓글 수
ggplot(wday_point, aes(x=요일, y=댓글수)) +
geom_col(fill="Coral", alpha=0.7, color="grey20") +
geom_text(aes(y=댓글수+130, label=댓글수), color='grey20', family="jalnan") +
theme_bw(base_family = "jalnan", base_size = 12) +
labs(title="요일별 댓글 수",y="댓글 수(단위:건)", subtitle="2019-05-23 ~ 2019-07-04") +
theme(plot.title = element_text(hjust=0.5),
plot.subtitle = element_text(hjust=0.5, color="grey20"),
axis.text.x = element_text(vjust=0.5),
axis.text.y = element_blank())
all_time_point <- dataset %>% # 데이터 생성
group_by(시간) %>%
summarise(date_mean_point = mean(점수),
공감수 = sum(공감),
비공감수 = sum(비공감),
댓글수 = n())
ggplot(all_time_point,aes(x=시간)) +
geom_point(aes(y=date_mean_point, color=rainbow(24), size=2)) +
geom_segment(aes(x=시간,xend=시간,y=0,yend=date_mean_point), color="grey50") +
coord_cartesian(ylim = c(9, 10)) +
theme_bw(base_family = "jalnan", base_size = 12) +
theme(legend.position = "none",
plot.title = element_text(hjust=0.5, size=20, family = "jalnan")) +
labs(y="평점평균", title="시간대별 평점평균") +
geom_text(aes(y=date_mean_point+0.08, label=round(date_mean_point,2), family="dohyeon"))
# 시간대별 댓글 수
ggplot(all_time_point, aes(x=시간, y=댓글수)) +
geom_col(fill="gold2", alpha=0.7, color="grey20") +
geom_text(aes(y=댓글수+50, label=댓글수), color='grey20', family="jalnan", size=3) +
theme_bw(base_family = "jalnan", base_size = 12) +
labs(title="시간대별 댓글 수",y="댓글 수(단위:건)", subtitle="2019-05-23 ~ 2019-07-04") +
theme(plot.title = element_text(hjust=0.5),
plot.subtitle = element_text(hjust=0.5, color="grey20"),
axis.text.x = element_text(vjust=0.5),
axis.text.y = element_blank())
