tdm
Encoding(tdm$dimnames$Terms) <- 'UTF-8'
tdm$dimnames$Terms
## 점수대(그룹)별 워드클라우드 --------------------------------------
clean <- function(text) {
text <- as.character(text)
text2 <- sapply(text, extractNoun, USE.NAMES = F)
text3 <- unlist(text2) %>% str_match("([가-힣]+)")
text4 <- Filter(function(x){nchar(x) >= 2 & nchar(x) <= 7},text3)
}
C_review <- clean(C_group$리뷰) # 5점이하
C_review
install.packages("tm")
install.packages("tm")
library(tm)
cps <- Corpus(VectorSource(C_group$리뷰))
ko.words <- function(doc){
d <- as.character(doc)
pos <- paste(SimplePos09(d))
extracted <- str_match(pos, '([가-힣]+)/[NP]')
keyword <- extracted[,2]
keyword[!is.na(keyword)]
}
tdm <- TermDocumentMatrix(cps, control=list(tokenize=ko.words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
weighting=weightBin))
tdm$dimnames$Terms
library(stringr)
words <- function(word){
text <- as.character(word)
text2 <- sapply(text, extractNoun, USE.NAMES = F)
text3 <- str_match(text2, "([가-힣]+)")
text4 <- Filter(function(x){nchar(x) >= 2 & nchar(x) <= 7}, text3)
gsub_txt <- readLines("gsubfile.txt")
for (i in 1:length(gsub_txt)) {
text5  <- gsub((gsub_txt[i]),"",text4)
}
}
tdm <- TermDocumentMatrix(cps, control=list(tokenize=words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
wordLengths=c(4, 10),
weighting=weightBin))
tdm$dimnames$Terms
dim(tdm)
tdm.matrix <- as.matrix(tdm)
rownames(tdm.matrix)[1:100]
word.count <- rowSums(tdm.matrix)  ##각 단어별 합계를 구함
word.order <- order(word.count, decreasing=T)  #다음으로 단어들을 쓰인 횟수에 따라 내림차순으로 정렬
freq.words <- tdm.matrix[word.order[1:20], ] #Term Document Matrix에서 자주 쓰인 단어 상위 20개에 해당하는 것만 추출
co.matrix <- freq.words %*% t(freq.words)  #행렬의 곱셈을 이용해 Term Document Matrix를 Co-occurence Matrix로 변경
install.packages("qgraph")
library(qgraph)
qgraph(co.matrix,
labels=rownames(co.matrix),   ##label 추가
diag=F,                       ## 자신의 관계는 제거함
layout='spring',              ##노드들의 위치를 spring으로 연결된 것 처럼 관련이 강하면 같이 붙어 있고 없으면 멀리 떨어지도록 표시됨
vsize=log(diag(co.matrix))*2) ##diag는 matrix에서 대각선만 뽑는 것임. 즉 그 단어가 얼마나 나왔는지를 알 수 있음. vsize는 그 크기를 결정하는데 여기 인자값으로 단어가 나온 숫자를 넘겨주는 것임. log를 취한것은 단어 크기의 차이가 너무 커서 log를 통해서 그 차이를 좀 줄여준것임.
words <- function(word){
text <- as.character(word)
text2 <- sapply(text, extractNoun, USE.NAMES = F)
text3 <- str_match(text2, "([가-힣]+)")
text4 <- Filter(function(x){nchar(x) >= 2 & nchar(x) <= 7}, text3)
gsub_txt <- readLines("gsubfile_C.txt")
for (i in 1:length(gsub_txt)) {
text5  <- gsub((gsub_txt[i]),"",text4)
}
}
tdm <- TermDocumentMatrix(cps, control=list(tokenize=words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
wordLengths=c(4, 10),
weighting=weightBin))
word.count <- rowSums(tdm.matrix)  ##각 단어별 합계를 구함
word.order <- order(word.count, decreasing=T)  #다음으로 단어들을 쓰인 횟수에 따라 내림차순으로 정렬
freq.words <- tdm.matrix[word.order[1:20], ] #Term Document Matrix에서 자주 쓰인 단어 상위 20개에 해당하는 것만 추출
co.matrix <- freq.words %*% t(freq.words)  #행렬의 곱셈을 이용해 Term Document Matrix를 Co-occurence Matrix로 변경
word.order
freq.words
co.matrix
words <- function(word){
text <- as.character(word)
text2 <- sapply(text, extractNoun, USE.NAMES = F)
text3 <- str_match(text2, "([가-힣]+)")
text4 <- Filter(function(x){nchar(x) >= 2 & nchar(x) <= 7}, text3)
gsub_txt <- readLines("gsubfile_C.txt")
for (i in 1:length(gsub_txt)) {
text4  <- gsub((gsub_txt[i]),"",text4)
}
}
tdm <- TermDocumentMatrix(cps, control=list(tokenize=words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
wordLengths=c(4, 10),
weighting=weightBin))
tdm.matrix <- as.matrix(tdm)
rownames(tdm.matrix)[1:100]
word.count <- rowSums(tdm.matrix)  ##각 단어별 합계를 구함
word.order <- order(word.count, decreasing=T)  #다음으로 단어들을 쓰인 횟수에 따라 내림차순으로 정렬
freq.words <- tdm.matrix[word.order[1:20], ] #Term Document Matrix에서 자주 쓰인 단어 상위 20개에 해당하는 것만 추출
co.matrix <- freq.words %*% t(freq.words)  #행렬의 곱셈을 이용해 Term Document Matrix를 Co-occurence Matrix로 변경
qgraph(co.matrix,
labels=rownames(co.matrix),   ##label 추가
diag=F,                       ## 자신의 관계는 제거함
layout='spring',              ##노드들의 위치를 spring으로 연결된 것 처럼 관련이 강하면 같이 붙어 있고 없으면 멀리 떨어지도록 표시됨
vsize=log(diag(co.matrix))*2) ##diag는 matrix에서 대각선만 뽑는 것임. 즉 그 단어가 얼마나 나왔는지를 알 수 있음. vsize는 그 크기를 결정하는데 여기 인자값으로 단어가 나온 숫자를 넘겨주는 것임. log를 취한것은 단어 크기의 차이가 너무 커서 log를 통해서 그 차이를 좀 줄여준것임.
gsub_txt
text4
ko.words <- function(doc){
d <- as.character(doc)
pos <- paste(SimplePos09(d))
extracted <- str_match(pos, '([가-힣]+)/[NP]')
keyword <- extracted[,2]
keyword[!is.na(keyword)]
}
cps <- Corpus(VectorSource(C_group$리뷰))
tdm <- TermDocumentMatrix(cps,
control=list(tokenize=ko.words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
wordLengths=c(2, 6),
weighting=weightBin))
dim(tdm)
tdm.matrix <- as.matrix(tdm)
rownames(tdm.matrix)[1:100]
word.count <- rowSums(tdm.matrix)  ##각 단어별 합계를 구함
word.order <- order(word.count, decreasing=T)  #다음으로 단어들을 쓰인 횟수에 따라 내림차순으로 정렬
freq.words <- tdm.matrix[word.order[1:20], ] #Term Document Matrix에서 자주 쓰인 단어 상위 20개에 해당하는 것만 추출
co.matrix <- freq.words %*% t(freq.words)  #행렬의 곱셈을 이용해 Term Document Matrix를 Co-occurence Matrix로 변경
install.packages("qgraph")
library(qgraph)
qgraph(co.matrix,
labels=rownames(co.matrix),   ##label 추가
diag=F,                       ## 자신의 관계는 제거함
layout='spring',              ##노드들의 위치를 spring으로 연결된 것 처럼 관련이 강하면 같이 붙어 있고 없으면 멀리 떨어지도록 표시됨
vsize=log(diag(co.matrix))*2) ##diag는 matrix에서 대각선만 뽑는 것임. 즉 그 단어가 얼마나 나왔는지를 알 수 있음. vsize는 그 크기를 결정하는데 여기 인자값으로 단어가 나온 숫자를 넘겨주는 것임. log를 취한것은 단어 크기의 차이가 너무 커서 log를 통해서 그 차이를 좀 줄여준것임.
tdm <- TermDocumentMatrix(cps,
control=list(tokenize=ko.words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
wordLengths=c(2, 6),
gsub_txt <- readLines("gsubfile_C.txt"),
for (i in 1:length(gsub_txt)) {
A_text  <- gsub((gsub_txt[i]),"",A_text)
}
weighting=weightBin))
tdm <- TermDocumentMatrix(cps,
control=list(tokenize=ko.words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
wordLengths=c(2, 6),
gsub_txt <- readLines("gsubfile_C.txt"),
for (i in 1:length(gsub_txt)) {
A_text  <- gsub((gsub_txt[i]),"",A_text)
},
weighting=weightBin))
tdm <- TermDocumentMatrix(cps,
control=list(tokenize=ko.words,   ## token 분류시 활용할 함수명 지정
removePunctuation=T,
removeNumbers=T,
wordLengths=c(2, 6),
=weightBin))
library(rJava)
library(KoNLP)
library(xlsx)
library(lubridate)
library(dplyr)
library(stringr)
library(wordcloud2)
library(reshape2)
library(extrafont)
windowsFonts(dohyeon=windowsFont("BM DoHyeon"))
windowsFonts(jalnan=windowsFont("Jalnan"))
library(ggplot2)
A_text <- readLines("알라딘_리뷰_전처리_후_A.txt")
head(sort(table(A_text), decreasing = T),50)
A_text <- gsub("재밌어요","재미", A_text)
A_text <- gsub("재밌고","재미", A_text)
A_text <- gsub("재밌게","재미", A_text)
A_text <- gsub("쟈스민","자스민", A_text)
A_text <- gsub("지니가","지니", A_text)
A_text <- gsub("윌스미스가","윌스미스", A_text)
A_text <- gsub("월스미스","윌스미스", A_text)
A_text <- gsub("관람객너무","관람객", A_text)
gsub_txt <- readLines("gsubfile_A.txt")
for (i in 1:length(gsub_txt)) {
A_text  <- gsub((gsub_txt[i]),"",A_text)
}
A_text <- A_text[A_text != ""]
A_wc <-head(sort(table(A_text), decreasing = T),50)
wordcloud2(A_wc, size=0.7,
col="random-light", backgroundColor="black",
fontFamily='나눔바른고딕')
wordcloud2(A_wc, size=0.7,
col="random-light", backgroundColor="black",
fontFamily='나눔바른고딕')
C_wc
# 워드클라우드 생성
wordcloud2(C_wc, size=0.6,
col="random-light", backgroundColor="black",
fontFamily='나눔바른고딕')
# C 그룹의 워드클라우드
C_text <- readLines("알라딘_리뷰_전처리_후_C.txt")
C_text <- gsub("애니메이션을","애니메이션",C_text)
C_text <- gsub("애니메이션이","애니메이션",C_text)
gsub_txt <- readLines("gsubfile_C.txt")
for (i in 1:length(gsub_txt)) {
C_text  <- gsub((gsub_txt[i]),"",C_text)
}
C_text <- C_text[C_text != ""]
C_wc <-head(sort(table(C_text), decreasing = T),50)
# 워드클라우드 생성
wordcloud2(C_wc, size=0.6,
col="random-light", backgroundColor="black",
fontFamily='나눔바른고딕')
# 워드클라우드 생성
wordcloud2(C_wc, size=0.6,
col="random-light", backgroundColor="black",
fontFamily='나눔바른고딕')
library(rJava)
library(KoNLP)
library(xlsx)
library(lubridate)
library(stringr)
library(wordcloud2)
library(reshape2)
library(extrafont)
windowsFonts(dohyeon=windowsFont("BM DoHyeon"))
windowsFonts(jalnan=windowsFont("Jalnan"))
library(ggplot2)
library(rvest)
library(dplyr)
trim <- function(text){gsub("\\s+","",text)} # str_trim(text, c("both"))와 같음
# 수집된 데이터 불러오기 --------------------------------
OG_data <- read.xlsx2("네이버_영화_리뷰_알라딘.xlsx", sheetIndex=1, header=T, stringsAsFactors=F)
dataset <- OG_data # 원본데이터 복사 (엑셀파일 불러올 때 메모리오류로 인해 에러가 자주남) # options(java.parameters = "-Xmx7168m")
dataset$점수 <- as.numeric(dataset$점수)
dataset$공감 <- as.numeric(dataset$공감)
dataset$비공감 <- as.numeric(dataset$비공감)
dataset$날짜 <- ymd(dataset$날짜)
dataset$시간 <- factor(dataset$시간, levels=c(0:23), order=T)
dataset$요일 <- factor(dataset$요일, levels=c("월","화","수","목","금","토","일"), order=T)
dataset$평점그룹 <- ifelse(dataset$점수 >= 9, "A", ifelse(dataset$점수 <= 5, "C", "B"))
dataset$주 <- ifelse(dataset$요일 %in% c("토","일"), "주말", "주중")
str(dataset)
dis <-dataset %>%
group_by(점수) %>%
summarise(리뷰수 = n()) %>% as.data.frame()
ggplot(dis, aes(x=점수, y=리뷰수)) +
geom_bar(stat="identity", fill=rainbow(10), position = "stack") +
geom_text(aes(y=리뷰수+500, label=리뷰수), family='dohyeon', size=3.5) +
scale_x_continuous(breaks = c(seq(0,10,1))) +
theme_bw(base_size = 12, base_family = "jalnan") +
labs(title = "점수별 리뷰 분포", y = "리뷰 수(단위:건)") +
theme(plot.title = element_text(hjust = 0.5))
# 일별 평점 데이터 생성
all_date_point <- dataset %>%
group_by(날짜, 요일) %>%
summarise(date_mean_point = mean(점수),
공감수 = sum(공감),
비공감수 = sum(비공감),
댓글수 = n())
# 일별 평점 평균 변화
ggplot(all_date_point,aes(x=날짜)) +
geom_line(aes(y=date_mean_point), color="grey20", linetype=2) +
geom_point(aes(y=date_mean_point), color=rainbow(43), size=3, alpha=.8) +
geom_text(aes(y=date_mean_point+0.04, label=round(date_mean_point,2)), family="dohyeon", size=3) +
coord_cartesian(ylim = c(9, 9.8)) +
theme_bw(base_family = "jalnan", base_size = 12) +
labs(y="평균평점", title="일별 평점평균 변화", subtitle="2019-05-23 ~ 2019-07-04") +
theme(legend.title = element_blank(),
plot.title = element_text(hjust=0.5, size=20, family = "jalnan"),
plot.subtitle = element_text(hjust=0.5, color="grey20"),
axis.text.x = element_text(angle = 90, vjust=0.5)) +
scale_x_date(breaks = all_date_point$날짜, labels=str_sub(all_date_point$날짜,7,10))
# 일별 댓글 수
ggplot(all_date_point, aes(x=날짜, y=댓글수)) +
geom_col(fill="antiquewhite", alpha=0.7, color="grey20") +
geom_text(aes(y=댓글수+30, label=댓글수), color='grey20', family="jalnan", size=3) +
geom_text(data = filter(all_date_point, 요일 %in% c("토", "일")), aes(y=100, label=요일), color="red", family="jalnan") +
theme_bw(base_family = "jalnan", base_size = 12) +
labs(title="일별 댓글 수",y="댓글 수(단위:건)", subtitle="2019-05-23 ~ 2019-07-04") +
theme(plot.title = element_text(hjust=0.5),
plot.subtitle = element_text(hjust=0.5, color="grey20"),
axis.text.x = element_text(angle = 90, vjust=0.5),
axis.text.y = element_blank()) +
scale_x_date(breaks = all_date_point$날짜, labels=str_sub(all_date_point$날짜,7,10))
# 요일별 평점 데이터 생성
wday_point <- dataset %>%
group_by(요일) %>%
summarise(date_mean_point = mean(점수),
공감수 = sum(공감),
비공감수 = sum(비공감),
댓글수 = n())
# 요일별 댓글 수
ggplot(wday_point, aes(x=요일, y=댓글수)) +
geom_col(fill="Coral", alpha=0.7, color="grey20") +
geom_text(aes(y=댓글수+130, label=댓글수), color='grey20', family="jalnan") +
theme_bw(base_family = "jalnan", base_size = 12) +
labs(title="요일별 댓글 수",y="댓글 수(단위:건)", subtitle="2019-05-23 ~ 2019-07-04") +
theme(plot.title = element_text(hjust=0.5),
plot.subtitle = element_text(hjust=0.5, color="grey20"),
axis.text.x = element_text(vjust=0.5),
axis.text.y = element_blank())
all_time_point <- dataset %>% # 데이터 생성
group_by(시간) %>%
summarise(date_mean_point = mean(점수),
공감수 = sum(공감),
비공감수 = sum(비공감),
댓글수 = n())
ggplot(all_time_point,aes(x=시간)) +
geom_point(aes(y=date_mean_point, color=rainbow(24), size=2)) +
geom_segment(aes(x=시간,xend=시간,y=0,yend=date_mean_point), color="grey50") +
coord_cartesian(ylim = c(9, 10)) +
theme_bw(base_family = "jalnan", base_size = 12) +
theme(legend.position = "none",
plot.title = element_text(hjust=0.5, size=20, family = "jalnan")) +
labs(y="평점평균", title="시간대별 평점평균") +
geom_text(aes(y=date_mean_point+0.08, label=round(date_mean_point,2), family="dohyeon"))
# 시간대별 댓글 수
ggplot(all_time_point, aes(x=시간, y=댓글수)) +
geom_col(fill="gold2", alpha=0.7, color="grey20") +
geom_text(aes(y=댓글수+50, label=댓글수), color='grey20', family="jalnan", size=3) +
theme_bw(base_family = "jalnan", base_size = 12) +
labs(title="시간대별 댓글 수",y="댓글 수(단위:건)", subtitle="2019-05-23 ~ 2019-07-04") +
theme(plot.title = element_text(hjust=0.5),
plot.subtitle = element_text(hjust=0.5, color="grey20"),
axis.text.x = element_text(vjust=0.5),
axis.text.y = element_blank())
C_wc <-head(sort(table(C_text), decreasing = T),50)
# C 그룹의 워드클라우드
C_text <- readLines("알라딘_리뷰_전처리_후_C.txt")
C_text <- gsub("애니메이션을","애니메이션",C_text)
C_text <- gsub("애니메이션이","애니메이션",C_text)
gsub_txt <- readLines("gsubfile_C.txt")
for (i in 1:length(gsub_txt)) {
C_text  <- gsub((gsub_txt[i]),"",C_text)
}
C_text <- C_text[C_text != ""]
C_wc <-head(sort(table(C_text), decreasing = T),50)
# 워드클라우드 생성
wordcloud2(C_wc, size=0.6,
col="random-light", backgroundColor="black",
fontFamily='나눔바른고딕')
library(rJava) # rJava와 KoNLP는 다른 패키지보다 우선적으로 설치해주는 게 좋음
library(KoNLP)
library(lubridate)
library(stringr)
library(reshape2)
library(rvest)
library(dplyr)
library(xlsx)
library(ggplot2)
library(extrafont)
windowsFonts(dohyeon=windowsFont("BM DoHyeon"))
windowsFonts(jalnan=windowsFont("Jalnan"))
base_url_1 <- "https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=" # 바뀌지 않는 앞쪽 url
moviecode <- "163788"     # 영화코드
moviename <- "알라딘"     # 영화이름
type <- "after"           # 영화관람 후
base_url_2 <- "&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page=" # page를 포함한 뒤쪽 url
# 영화 '알라딘'의 url 생성
main_url <- paste0(base_url_1, moviecode)
total_page_num <- paste0(main_url, "&type=", type, base_url_2, "1") %>%
read_html() %>%
html_node(".score_total") %>%
html_node(".total") %>%
html_nodes("em") %>% .[2] %>% html_text() %>% str_remove(",") %>% as.numeric()/10
total_page_num
# page url 생성 -------------------------------------------------------------------------
page_url <- NULL
for (page in 1:ceiling(total_page_num)) { # 총 페이지 수를 올림
page_url[page] <- paste0(main_url, "&type=", type, base_url_2, page)
}
head(page_url); tail(page_url)
trim <- function(text){gsub("\\s+","",text)} # str_trim(text, c("both"))와 같음
reviews <- NULL
lis <- NULL
score <- NULL
user_name <- NULL
date <- NULL
time <- NULL
up <- NULL
down <- NULL
movie_code <- NULL
movie_name <- NULL
url_num <- 0
for (url in page_url) {
url_num <- url_num+length(url)
if(url_num%%10==0){
print(url_num)} # 출력중인 url 보이기 (10단위)
html <- read_html(url)
lis <- html %>% html_node("div.score_result") %>% html_nodes("li") # 공통 경로
reviews <- c(reviews, lis %>% # 리뷰
html_node(".score_reple") %>%
html_node("p") %>%
html_text())
user_name <- c(user_name, lis %>% # 작성자 닉네임
html_node(".score_reple") %>%
html_nodes("em") %>%
.[seq(1, length(lis)*3, by=3)] %>%
html_text() %>% trim())
date <-  c(date, lis %>% # 작성날짜 및 시간
html_node(".score_reple") %>%
html_nodes("em")  %>%
.[seq(2, length(lis)*3, by=3)] %>%
html_text() %>%
str_replace_all(.,"\\.+","-"))
score <- c(score, lis %>% # 평점
html_node(".star_score") %>%
html_text() %>%
trim())
up <- c(up, lis %>% # 공감 수
html_node(".btn_area") %>%
html_nodes("strong") %>%
.[grep("sympathy",.)] %>%
html_text())
down <- c(down, lis %>% # 비공감 수
html_node(".btn_area") %>%
html_nodes("strong") %>%
.[grep("notSympathy",.)] %>%
html_text())
movie_code <- c(movie_code, rep(moviecode, length(lis))) # 영화코드
movie_name <- c(movie_name, rep(moviename, length(lis))) # 영화이름
}
trim <- function(text){gsub("\\s+","",text)} # str_trim(text, c("both"))와 같음
reviews <- NULL
lis <- NULL
score <- NULL
user_name <- NULL
date <- NULL
time <- NULL
up <- NULL
down <- NULL
movie_code <- NULL
movie_name <- NULL
url_num <- 0
for (url in page_url) {
url_num <- url_num+length(url)
if(url_num%%10==0){
print(url_num)} # 출력중인 url 보이기 (10단위)
html <- read_html(url)
lis <- html %>% html_node("div.score_result") %>% html_nodes("li") # 공통 경로
reviews <- c(reviews, lis %>% # 리뷰
html_node(".score_reple") %>%
html_node("p") %>%
html_text())
user_name <- c(user_name, lis %>% # 작성자 닉네임
html_node(".score_reple") %>%
html_nodes("em") %>%
.[seq(1, length(lis)*3, by=3)] %>%
html_text() %>% trim())
date <-  c(date, lis %>% # 작성날짜 및 시간
html_node(".score_reple") %>%
html_nodes("em")  %>%
.[seq(2, length(lis)*3, by=3)] %>%
html_text() %>%
str_replace_all(.,"\\.+","-"))
score <- c(score, lis %>% # 평점
html_node(".star_score") %>%
html_text() %>%
trim())
up <- c(up, lis %>% # 공감 수
html_node(".btn_area") %>%
html_nodes("strong") %>%
.[grep("sympathy",.)] %>%
html_text())
down <- c(down, lis %>% # 비공감 수
html_node(".btn_area") %>%
html_nodes("strong") %>%
.[grep("notSympathy",.)] %>%
html_text())
movie_code <- c(movie_code, rep(moviecode, length(lis))) # 영화코드
movie_name <- c(movie_name, rep(moviename, length(lis))) # 영화이름
}
naver_movie_reviews <- data.frame(영화명=movie_name,
영화코드=movie_code,
이름=user_name,
리뷰=reviews,
점수=score,
공감=up,
비공감=down,
날짜=str_sub(date,1,10),
시간=hour(date),
요일=wday(date, label = T)) # weekdays 함수로 날짜에 맞는 요일 출력
naver_movie_reviews <- data.frame(영화명=movie_name,
영화코드=movie_code,
이름=user_name,
리뷰=reviews,
점수=score,
공감=up,
비공감=down,
날짜=str_sub(date,1,10),
시간=hour(date),
요일=wday(date, label = T)) # weekdays 함수로 날짜에 맞는 요일 출력
C_wc <-head(sort(table(C_text), decreasing = T),50)
# 워드클라우드 생성
wordcloud2(C_wc, size=0.6,
col="random-light", backgroundColor="black",
fontFamily='나눔바른고딕')
C_wc <-head(sort(table(C_text), decreasing = T),50)
# 워드클라우드 생성
wordcloud2(C_wc, size=0.6,
col="random-light", backgroundColor="black",
fontFamily='나눔바른고딕')
library(wordcloud2)
library(wordcloud2)
C_wc <-head(sort(table(C_text), decreasing = T),50)
# 워드클라우드 생성
wordcloud2(C_wc, size=0.6,
col="random-light", backgroundColor="black",
fontFamily='나눔바른고딕')
